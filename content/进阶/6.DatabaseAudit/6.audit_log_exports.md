---
title: "把审计日志导出到Elasticsearch以及S3"
chapter: false
weight: 63
tags:
  - advanced
---

本节将介绍以下内容

* [把CloudWatch日志发送到ElasticSearch](#es_audit_log)
* [利用Lambda定期转储日志到S3](#audit_log_lambda_to_s3)

## 把CloudWatch日志发送到ElasticSearch{#es_audit_log}
对于习惯用ElasticSearch的用户，可以通过CloudWatch的订阅功能，把日志准实时发送到Amazon ES。

**注意：将大量CloudWatch Logs数据流式传输到Amazon ES可能会产生较高的使用费。**

如果对日志传输的实时性没有要求，可以参考下一节“利用Lambda定期转储日志到S3”，把日志定期导出到S3，然后用ES分析S3的数据。

### 创建Elasticsearch 域
{{% notice warning %}}
此过程简要介绍了如何配置测试域。不应使用它来创建生产域。
{{% /notice  %}}

1. 进入Elasticsearch控制台，并创建Elasticsearch域：https://ap-northeast-1.console.aws.amazon.com/es/home?region=ap-northeast-1#create-domain 
2. 在步骤1的“**选择部署类型**”的页面上，选择“**开发和测试**”选项。“**Elasticsearch 版本**”保留缺省值，点击【**下一步**】按钮。
![](/images/6.DatabaseAudit/elasticsearch_creation1.png)
3. 在步骤2的“配置域”的页面上，“**Elasticsearch 域名**”设置为“**rdsaudites**”，“**每个节点的 EBS 存储大小**”部分设置为50GB，其他选项保留缺省值，点击【**下一步**】按钮。
![](/images/6.DatabaseAudit/elasticsearch_creation2.png)
4. 在步骤3的“**配置访问和安全**”页面上，“**网络配置**”选项中，选择“**VPC访问**”，具体“**VPC**”选择“**SecWorkshopVPC**”，“**子网**”选择“**PublicSubnet1**”，“**安全组**”选择“**SecWorkshopVPC-BastionSecurityGroup**”

对于“**精细访问控制**”，选择“**创建主用户**”单选框，主用户名输入“**admin**”，主密码和确认主密码输入“**P@ssw0rd**”。 

在“**域访问策略**”里，选择“**JSON定义的访问策略**”，输入下面的内容。其他保留缺省值。点击【**下一步**】按钮。
```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "*"
      },
      "Action": "es:*",
      "Resource": "*"
    }
  ]
}
```
![](/images/6.DatabaseAudit/elasticsearch_creation3.png)
5.在步骤4的“**审核**”页面上，点击【**确认**】按钮。等待Elasticsearch集群的创建，大约需要10分钟。
![](/images/6.DatabaseAudit/elasticsearch_creation4.png)

### 配置CloudWatch把日志准实时的传输到Elasticsearch域
1. 进入CloudWatch日志组控制台:https://ap-northeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-1#logsV2:log-groups$3FlogGroupNameFilter$3Dwordpress-cluster
选中“wordpress-cluster”的日志组，然后在【操作】下拉菜单里选中“**创建ElasticSearch订阅筛选条件**”
![订阅筛选条件](/images/6.DatabaseAudit/4.3.1.log-management-of-rds-mysql.png)
2. 在“选择目标”部分中，“选择账户”里选中”This account“单选框，在“Amazon ES cluster”下拉框里选择“rdsaudites”。
常见的日志格式以及空格分隔的CSV文件可以自动解析。但RDS审计日志是逗号分隔的CSV，需要在后面步骤的Lambda函数中解析。这里日志格式选**其它**，订阅筛选条件模式什么都不填，把所有日志都发送到ES。“订阅筛选条件名称”输入“rdsauditing”。
其他保留缺省值，点击【**开始流式传输**】按钮完成设置。
![配置](/images/6.DatabaseAudit/auditlog-to-es1.png)
3. 转到Lambda控制台：https://ap-northeast-1.console.aws.amazon.com/lambda/home?region=ap-northeast-1#/functions?f0=true&n0=false&op=and&v0=LogsToElasticsearch_rdsaudites
可以看到自动生产了一个Lambda函数做流式传输。
![Lambda函数](/images/6.DatabaseAudit/4.3.5.log-management-of-rds-mysql.png)
触发器和权限设定都已经自动设置好了。
![Lambda触发器](/images/6.DatabaseAudit/4.3.6.log-management-of-rds-mysql.png)
4. 对transform()方法里的第64行的“cwl”改为“rdsaudit”。
```javascript
//修改前
        var indexName = [
            'cwl-' + timestamp.getUTCFullYear(),              // year
            ('0' + (timestamp.getUTCMonth() + 1)).slice(-2),  // month
            ('0' + timestamp.getUTCDate()).slice(-2)          // day
        ].join('.');
//修改后
        var indexName = [
            'rdsaudit-' + timestamp.getUTCFullYear(),              // year
            ('0' + (timestamp.getUTCMonth() + 1)).slice(-2),  // month
            ('0' + timestamp.getUTCDate()).slice(-2)          // day
        ].join('.');
```

第69行到75行之间的代码进行修改，让Lambda函数解析出RDS审计日志的各个字段。
修改前后的内容如下，把日志内容@message解析成10个字段。拷贝修改后的内容到函数里以后，一定要注意缩进是否正确。
```javascript
//修改前
        var source = buildSource(logEvent.message, logEvent.extractedFields);
        source['@id'] = logEvent.id;
        source['@timestamp'] = new Date(1 * logEvent.timestamp).toISOString();
        source['@message'] = logEvent.message;
        source['@owner'] = payload.owner;
        source['@log_group'] = payload.logGroup;
        source['@log_stream'] = payload.logStream;

//修改后
        var source = buildSource(logEvent.message, logEvent.extractedFields);
        source['@id'] = logEvent.id;
        source['@timestamp'] = new Date(1 * logEvent.timestamp).toISOString();
        //source['@message'] = logEvent.message;
        var regex = /^([^,]+),([^,]+),([^,]+),([^,]+),(\d+),(\d+),(\w+),([^,]*),(.*),(\d+)$/;
        var reg_arr = logEvent.message.match(regex);
        //source['@time'] = reg_arr[1];
        source['@server'] = reg_arr[2];
        source['@user'] = reg_arr[3];
        source['@client'] = reg_arr[4];
        source['@conid'] = reg_arr[5];
        source['@qryid'] = reg_arr[6];
        source['@oper'] = reg_arr[7];
        source['@db'] = reg_arr[8];
        source['@sql'] = reg_arr[9];
        source['@code'] = reg_arr[10];
        source['@owner'] = payload.owner;
        source['@log_group'] = payload.logGroup;
        source['@log_stream'] = payload.logStream;
```
修改后的截图如下。修改完以后，点击【**部署**】按钮，让修改生效。
![Lambda代码](/images/6.DatabaseAudit/4.3.7.log-management-of-rds-mysql.png)
5. 可以在Lambda函数的**监控**页面查看执行情况。
![Lambda安全组](/images/6.DatabaseAudit/4.3.12.log-management-of-rds-mysql.png)


## 使用 Kibana 显示Elasticsearch里的数据

### 配置SSH隧道

进入EC2控制台，找到我们创建的跳板机，并把它的公网ip地址拷贝下来：https://ap-northeast-1.console.aws.amazon.com/ec2/v2/home?region=ap-northeast-1#Instances:instanceState=running;search=BastionInstance

进入Elastisearch控制台，记录Kibana的域名：https://ap-northeast-1.console.aws.amazon.com/es/home?region=ap-northeast-1#domain:resource=rdsaudites;action=dashboard;tab=undefined


下面介绍如何通过 SSH 隧道访问 VPC 内的 Kibana。如果是mac电脑，则执行下面的命令：
```
## 此 SSH 隧道将本地的 9200 端口代理到远端 Kibana URL 的443端口
ssh -i <你的姓名拼音首字母>-secworkshop.pem ec2-user@<跳板机的公网ip> -N -L 9200:<cloudtrailes集群的域名>:443
```
如果是windows电脑，请下载putty工具到本地，并启动该工具。该工具可以无需安装，直接执行。因为putty需要用到ppk格式的keypair，如果你是下载的pem格式，则使用puttygen工具进行转换。
{{%attachments title="下载链接:" /%}}

a.启动putty以后，从CloudFormation 输出中记录的 SSH 隧道连接方式中，输入ec2-user@<跳板机的公网ip>，如下图所示。
![](/images/2.OperationMonitor/putty1.png)

b.在左边点开“Connection”，再点开“SSH”，再点开“Auth”，在右边的“private key file”一栏，点击【Browse...】按钮，选择你的keypair文件：<你的姓名拼音首字母>-secworkshop。
![](/images/2.OperationMonitor/putty2.png)

c.在左边点击“Tunnels”，在右边的“Source port”里，输入端口：9200。

然后在Destination一栏输入Kibana的域名，可以从CloudFormation 输出中记录的 SSH 隧道连接方式中找到，比如：vpc-secworkshop-c4agv6xpwokgy7zqq5avlfpgli.us-east-1.es.amazonaws.com:443
![](/images/2.OperationMonitor/putty3.png)

d.点击【Add】按钮，把source port和destination的内容添加到列表里。
![](/images/6.DatabaseAudit/putty4.png)

e.点击【Open】按钮，建立到跳板机的SSH隧道。

## Kibana 配置
1. 在本地浏览器中，打开```https://localhost:9200/_plugin/kibana/```，输入创建Elasticsearch集群时所指定的用户名和密码，默认为 ```admin / P@ssw0rd``` .
![](/images/3.NetworkSecurity/Cf-realtimelog-KibanaLogin.png)
2. 如果有弹出欢迎页面，点击 Explore on my own
3. 点击左上角菜单按钮，选择 **Security**
4. 点击左侧 **Roles** 并在role列表中点击 all_access 链接
5. 在新页面中点击上方**Mapped users**, 然后点击右侧 **Manage mapping**
6. 在 **Backend Roles** 中将 lambda_cloudtrailes_role 的ARN （可以通过IAM控制台：https://console.aws.amazon.com/iam/home?region=ap-northeast-1#/roles/lambda_cloudtrailes_role
进行拷贝）粘贴到文本框中,然后点击右下角 **Map**按钮
![](/images/2.OperationMonitor/kinbana_role.png)
7. 点击左上角菜单按钮，点击 **Discover**，此时系统提示没有index信息
8. 点击右上角 **Create index pattern** 按钮
9. 在**Index pattern name** 中输入 rdsaudit-*, 点击 【Next step】
10. 在 Time field 中选择 @timestamp， 并点击 **Create index pattern**
11. 系统显示所有cloudtrail 字段
![](/images/6.DatabaseAudit/kinbana_discover.png)
12. 点击左上角菜单按钮，点击 **Discover**, 确认日志信息已经进入ES

### 利用Lambda定期转储日志到S3{#audit_log_lambda_to_s3}
在对监控实时性要求不高，可以通过Lambda定期转储RDS日志的方式降低成本。

AWS Lambda是无服务器计算服务，可运行代码来响应事件并自动管理底层计算资源。它非常适合在一天导出几次日志这样的场景来使用。

#### 代码说明

1. 这段代码利用API获得RDS的日志状态。其中**describeDBLogFiles**获得1小时内新生成的日志一览后依次处理各个日志。**downloadDBLogFilePortion**下载日志并压缩后存储到Lambda的/tmp临时路径，**putObject**上传日志文件到S3存储桶。

2. Lambda代码最多运行15分钟，处理能力与分配的内存大小有关。建议分配**256MiB**以上的内存并指定**15分钟**的超时时间。如果RDS在业务繁忙时间生成的日志量太大，Lambda不能在15分钟内处理完所有日志，就需要分配更大的内存。

3. Lambda的临时路径/tmp的容量是512MiB，以20%的压缩比率计算，能处理最大**2.5GiB**的单个日志。

4. Lambda除了基本的执行权限以外，还需要**内联策略**下载RDS日志，附加**AWSLambdaExecute**权限上传到S3。

5. 日志会储存到指定的存储桶，路径结构：[bucket]–RDS–[rds_id]–[log_type]–[YYYY-MM-DD]
![S3路径结构](/images/6.DatabaseAudit/4.4.1.log-management-of-rds-mysql.png)

#### 创建Lambda的步骤
1. 从头创建lambda 函数，进入控制台：https://ap-northeast-1.console.aws.amazon.com/lambda/home?region=ap-northeast-1#/create/function
![创建函数](/images/6.DatabaseAudit/4.4.2.log-management-of-rds-mysql.png)
这个步骤将创建一个拥有基本的Lambda执行权限的角色。后面第4步中再给它添加RDS和S3的访问权限。
2. 配置一个CloudWatch Events，每小时的第10分钟自动执行Lambda
Schedule可以用Crontab格式：cron(10 * ? * * *)
![添加触发器](/images/6.DatabaseAudit/4.4.3.log-management-of-rds-mysql.png)
![添加触发器](/images/6.DatabaseAudit/4.4.4.log-management-of-rds-mysql.png)
![添加触发器](/images/6.DatabaseAudit/4.4.5.log-management-of-rds-mysql.png)
3. 复制示例代码到函数代码框
Lambda示例代码(**NodeJS 12**)：
下面的代码里面，rdsid、bucket、region三个变量需要根据实际情况修改。比如这里的bucket为“<你的姓名拼音首字母-secworkshop>”，region为“ap-northeast-1”，rdsid可以通过控制台找到“角色”为“写入器”对应的实例id：https://ap-northeast-1.console.aws.amazon.com/rds/home?region=ap-northeast-1#database:id=wordpress-cluster;is-cluster=true;tab=connectivity
```javascript
const zlib = require('zlib');
const fs = require('fs');
const rdsid = '<your rdsid>';
const bucket = '<your bucket>';
const AWS = require('aws-sdk');
AWS.config.update({region: 'ap-northeast-1'});

function listLog(context) {

        let params = {
                DBInstanceIdentifier: rdsid,
                // filter for non 0-byte file rotated in 1 hour
                FileLastWritten: Date.now() - 3600000,
                FileSize: '1',
                FilenameContains: 'log.'
        };

        let request = new AWS.RDS().describeDBLogFiles(params);
        request.on('success', (response) => {
                console.log(response.data);
                getRdsLog(context, response.data.DescribeDBLogFiles, '0');
        }).on('error', (err) => {
                context.fail(err);
        }).send();
}

function getRdsLog(context, logArray, marker) {

        if (!logArray[0]) return;
        let logObj = logArray[0];
        let s3name = logObj.LogFileName.slice(0,5) + '-' + rdsid + '-' + logObj.LastWritten + '.log.gz';

        let params = {
                DBInstanceIdentifier: rdsid,
                LogFileName: logObj.LogFileName,
                Marker: marker
        };

        let request = new AWS.RDS().downloadDBLogFilePortion(params);
        request.on('success', (response) => {
                console.log(s3name + ' ' + response.data.Marker);
                try {
                        fs.appendFileSync('/tmp/' + s3name, zlib.gzipSync(response.data.LogFileData));
                }
                catch (err) {
                        context.fail(err);
                };
                if (response.data.AdditionalDataPending) {
                        getRdsLog(context, logArray, response.data.Marker);
                }
                else {
                        console.log(s3name + ' downloaded');
                        logArray.shift();
                        putLog(context, s3name, logArray);
                }
        }).on('error', (err) => {
                context.fail(err);
        }).send();
}

function putLog(context, s3name, logArray) {

        let params = {
                Body: fs.readFileSync('/tmp/' + s3name),
                Bucket: bucket,
                ContentType: 'text/plain',
                ContentEncoding: 'gzip',
                ServerSideEncryption: 'AES256',
                Key: 'RDS/' + rdsid + '/' + s3name.slice(0, 5) + '/' + new Date().toISOString().slice(0,10) + '/' + s3name
        };

        let request = new AWS.S3().putObject(params);
        request.on('success', () => {
                console.log(s3name + ' uploaded to S3');
                fs.unlinkSync('/tmp/' + s3name);
                getRdsLog(context, logArray, '0');
        }).on('error', (err) => {
                context.fail(err);
        }).send();
}

exports.handler = function(event, context) {

        listLog(context);
};
```

![保存函数](/images/6.DatabaseAudit/4.4.6.log-management-of-rds-mysql.png)
粘贴和修改完代码以后，要点击一下画面右上角的**保存**按钮。

4. 编辑**基本设置**中的**内存**和**超时**
![编辑基本设置](/images/6.DatabaseAudit/4.4.7.log-management-of-rds-mysql.png)

权限设置，点击下方**查看…角色**的链接，为RDS和S3的相关操作**附加策略**。权限设置完成后再点**保存**。
![角色摘要](/images/6.DatabaseAudit/4.4.8.log-management-of-rds-mysql.png)

先附加策略**AWSLambdaExecute**
![附加权限](/images/6.DatabaseAudit/4.4.9.log-management-of-rds-mysql.png)

再添加**内联策略**
![创建策略](/images/6.DatabaseAudit/4.4.10.log-management-of-rds-mysql.png)

在JSON窗口粘贴以下内容，创建一个名为LambdaDownloadRDSLog的策略

```config
{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Action": [
            "rds:DescribeDBLogFiles",
            "rds:DownloadDBLogFilePortion"
        ], "Resource": "*"
    }
  ]
}
```
![查看策略](/images/6.DatabaseAudit/4.4.11.log-management-of-rds-mysql.png)
![附加策略](/images/6.DatabaseAudit/4.4.12.log-management-of-rds-mysql.png)
5. 测试Lambda函数

这个Lambda函数是定时驱动的，不需要传入参数。点击**测试**按钮后用默认事件模版**创建**一个测试事件，然后再**测试**。
![配置测试事件](/images/6.DatabaseAudit/4.4.13.log-management-of-rds-mysql.png)
![配置测试事件](/images/6.DatabaseAudit/4.4.14.log-management-of-rds-mysql.png)
![执行结果](/images/6.DatabaseAudit/4.4.15.log-management-of-rds-mysql.png)

执行结果的详细信息里面，会列出**describeDBLogFiles**获得的1小时内新生成的日志一览，**downloadDBLogFilePortion**分页下载日志的结果，**putObject**上传日志文件到S3存储桶的结果。
![查看日志](/images/6.DatabaseAudit/4.4.16.log-management-of-rds-mysql.png)

Lambda函数测试通过以后，就可以**保存**下来。这个函数会被刚才配置的CloudWatch事件定期触发，自动转储日志到S3存储桶。

